{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malak-elbanna/NN_from_scratch/blob/main/NN_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WeXnIwgAIfa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRb6EvUCqvCX"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgM5m4Hoy-De",
        "outputId": "11a35eeb-2ae1-4f19-d876-cf7b1bff807c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (60000, 28, 28)\n",
            "y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "y_test:  (10000,)\n"
          ]
        }
      ],
      "source": [
        "print('X_train: ' + str(X_train.shape))\n",
        "print('y_train: ' + str(y_train.shape))\n",
        "print('X_test:  '  + str(X_test.shape))\n",
        "print('y_test:  '  + str(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUznA9_F5Bi5"
      },
      "source": [
        "#Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r5TVUsO4gsy"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2zIBLHc5OD2"
      },
      "source": [
        "#Convert 2D 28x28 pixel imgs to 784 pixel 1D array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYUNhmSQ5NTH"
      },
      "outputs": [],
      "source": [
        "X_train_f = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_f = X_test.reshape(X_test.shape[0], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdKU5UKp5snE"
      },
      "source": [
        "#OneHotEncoder from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPaCZUS65gTb"
      },
      "outputs": [],
      "source": [
        "one_hot = np.zeros((y_train.shape[0], 10))\n",
        "for i, label in enumerate(y_train):\n",
        "  one_hot[i, label] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq19TEPd6Pi6"
      },
      "source": [
        "#ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PaWU5Ph60PT"
      },
      "outputs": [],
      "source": [
        "def relu(z):\n",
        "    return np.maximum(0, z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIGFhvOuHVYk"
      },
      "source": [
        "#Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM2tveqQFREW"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
        "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiPPB0gW_OLW"
      },
      "source": [
        "#Initialize Weights and Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TedA1w4Z7XKN"
      },
      "outputs": [],
      "source": [
        "def he_initialization(size_layer, size_next_layer):\n",
        "    return np.random.randn(size_next_layer, size_layer) * np.sqrt(2. / size_layer)\n",
        "\n",
        "input_layer = 784\n",
        "hidden_layer = 532\n",
        "output_layer = 10\n",
        "\n",
        "W_hidden = he_initialization(input_layer, hidden_layer)\n",
        "b_hidden = np.zeros((hidden_layer, 1))\n",
        "\n",
        "W_output = he_initialization(hidden_layer, output_layer)\n",
        "b_output = np.zeros((output_layer, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxA2rG-lMsQ0"
      },
      "source": [
        "#Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJAxrMuDB9rq"
      },
      "outputs": [],
      "source": [
        "def forward(X):\n",
        "    Z_hidden = np.dot(W_hidden, X.T) + b_hidden\n",
        "    hidden_relu = relu(Z_hidden)\n",
        "\n",
        "    Z_output = np.dot(W_output, hidden_relu) + b_output\n",
        "    output_softmax = softmax(Z_output)\n",
        "\n",
        "    return output_softmax, hidden_relu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5byMObZUSxP"
      },
      "source": [
        "#Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSKalia3SO5u"
      },
      "outputs": [],
      "source": [
        "def backpropagation(X, one_hot, output, hidden_relu):\n",
        "    error_output = output - one_hot.T\n",
        "\n",
        "    gradient_Woutput = np.dot(error_output, hidden_relu.T) / X.shape[0]\n",
        "    gradient_boutput = np.sum(error_output, axis=1, keepdims=True) / X.shape[0]\n",
        "\n",
        "    error_hidden = np.dot(W_output.T, error_output) * (hidden_relu > 0)\n",
        "\n",
        "    gradient_Whidden = np.dot(error_hidden, X) / X.shape[0]\n",
        "    gradient_bhidden = np.sum(error_hidden, axis=1, keepdims=True) / X.shape[0]\n",
        "\n",
        "    return gradient_Woutput, gradient_boutput, gradient_Whidden, gradient_bhidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fSyBycuXJKw"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OahAUf5sVml3"
      },
      "outputs": [],
      "source": [
        "def train(X_train_f, one_hot, n_epochs, lr):\n",
        "    global W_output, b_output, W_hidden, b_hidden\n",
        "    for epoch in range(n_epochs):\n",
        "        output, hidden_relu = forward(X_train_f)\n",
        "\n",
        "        grad_Woutput, grad_boutput, grad_Whidden, grad_bhidden = backpropagation(X_train_f, one_hot, output, hidden_relu)\n",
        "\n",
        "        W_output -= lr * grad_Woutput\n",
        "        b_output -= lr * grad_boutput\n",
        "        W_hidden -= lr * grad_Whidden\n",
        "        b_hidden -= lr * grad_bhidden\n",
        "\n",
        "        loss = -np.mean(np.sum(one_hot * np.log(output.T + 1e-8), axis=1))\n",
        "        print(f\"Epoch {epoch + 1}/{n_epochs} - Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vDRXCr9YFRf",
        "outputId": "b7c2df5a-d70f-4a93-c4f8-21c3555b3021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200 - Loss: 0.3738\n",
            "Epoch 2/200 - Loss: 0.3736\n",
            "Epoch 3/200 - Loss: 0.3734\n",
            "Epoch 4/200 - Loss: 0.3731\n",
            "Epoch 5/200 - Loss: 0.3729\n",
            "Epoch 6/200 - Loss: 0.3727\n",
            "Epoch 7/200 - Loss: 0.3724\n",
            "Epoch 8/200 - Loss: 0.3722\n",
            "Epoch 9/200 - Loss: 0.3720\n",
            "Epoch 10/200 - Loss: 0.3717\n",
            "Epoch 11/200 - Loss: 0.3715\n",
            "Epoch 12/200 - Loss: 0.3713\n",
            "Epoch 13/200 - Loss: 0.3711\n",
            "Epoch 14/200 - Loss: 0.3708\n",
            "Epoch 15/200 - Loss: 0.3706\n",
            "Epoch 16/200 - Loss: 0.3704\n",
            "Epoch 17/200 - Loss: 0.3702\n",
            "Epoch 18/200 - Loss: 0.3699\n",
            "Epoch 19/200 - Loss: 0.3697\n",
            "Epoch 20/200 - Loss: 0.3695\n",
            "Epoch 21/200 - Loss: 0.3693\n",
            "Epoch 22/200 - Loss: 0.3690\n",
            "Epoch 23/200 - Loss: 0.3688\n",
            "Epoch 24/200 - Loss: 0.3686\n",
            "Epoch 25/200 - Loss: 0.3684\n",
            "Epoch 26/200 - Loss: 0.3682\n",
            "Epoch 27/200 - Loss: 0.3679\n",
            "Epoch 28/200 - Loss: 0.3677\n",
            "Epoch 29/200 - Loss: 0.3675\n",
            "Epoch 30/200 - Loss: 0.3673\n",
            "Epoch 31/200 - Loss: 0.3671\n",
            "Epoch 32/200 - Loss: 0.3669\n",
            "Epoch 33/200 - Loss: 0.3666\n",
            "Epoch 34/200 - Loss: 0.3664\n",
            "Epoch 35/200 - Loss: 0.3662\n",
            "Epoch 36/200 - Loss: 0.3660\n",
            "Epoch 37/200 - Loss: 0.3658\n",
            "Epoch 38/200 - Loss: 0.3656\n",
            "Epoch 39/200 - Loss: 0.3654\n",
            "Epoch 40/200 - Loss: 0.3652\n",
            "Epoch 41/200 - Loss: 0.3649\n",
            "Epoch 42/200 - Loss: 0.3647\n",
            "Epoch 43/200 - Loss: 0.3645\n",
            "Epoch 44/200 - Loss: 0.3643\n",
            "Epoch 45/200 - Loss: 0.3641\n",
            "Epoch 46/200 - Loss: 0.3639\n",
            "Epoch 47/200 - Loss: 0.3637\n",
            "Epoch 48/200 - Loss: 0.3635\n",
            "Epoch 49/200 - Loss: 0.3633\n",
            "Epoch 50/200 - Loss: 0.3631\n",
            "Epoch 51/200 - Loss: 0.3629\n",
            "Epoch 52/200 - Loss: 0.3627\n",
            "Epoch 53/200 - Loss: 0.3625\n",
            "Epoch 54/200 - Loss: 0.3623\n",
            "Epoch 55/200 - Loss: 0.3621\n",
            "Epoch 56/200 - Loss: 0.3619\n",
            "Epoch 57/200 - Loss: 0.3617\n",
            "Epoch 58/200 - Loss: 0.3615\n",
            "Epoch 59/200 - Loss: 0.3613\n",
            "Epoch 60/200 - Loss: 0.3611\n",
            "Epoch 61/200 - Loss: 0.3609\n",
            "Epoch 62/200 - Loss: 0.3607\n",
            "Epoch 63/200 - Loss: 0.3605\n",
            "Epoch 64/200 - Loss: 0.3603\n",
            "Epoch 65/200 - Loss: 0.3601\n",
            "Epoch 66/200 - Loss: 0.3599\n",
            "Epoch 67/200 - Loss: 0.3597\n",
            "Epoch 68/200 - Loss: 0.3595\n",
            "Epoch 69/200 - Loss: 0.3593\n",
            "Epoch 70/200 - Loss: 0.3591\n",
            "Epoch 71/200 - Loss: 0.3589\n",
            "Epoch 72/200 - Loss: 0.3587\n",
            "Epoch 73/200 - Loss: 0.3585\n",
            "Epoch 74/200 - Loss: 0.3583\n",
            "Epoch 75/200 - Loss: 0.3581\n",
            "Epoch 76/200 - Loss: 0.3580\n",
            "Epoch 77/200 - Loss: 0.3578\n",
            "Epoch 78/200 - Loss: 0.3576\n",
            "Epoch 79/200 - Loss: 0.3574\n",
            "Epoch 80/200 - Loss: 0.3572\n",
            "Epoch 81/200 - Loss: 0.3570\n",
            "Epoch 82/200 - Loss: 0.3568\n",
            "Epoch 83/200 - Loss: 0.3566\n",
            "Epoch 84/200 - Loss: 0.3564\n",
            "Epoch 85/200 - Loss: 0.3563\n",
            "Epoch 86/200 - Loss: 0.3561\n",
            "Epoch 87/200 - Loss: 0.3559\n",
            "Epoch 88/200 - Loss: 0.3557\n",
            "Epoch 89/200 - Loss: 0.3555\n",
            "Epoch 90/200 - Loss: 0.3553\n",
            "Epoch 91/200 - Loss: 0.3551\n",
            "Epoch 92/200 - Loss: 0.3550\n",
            "Epoch 93/200 - Loss: 0.3548\n",
            "Epoch 94/200 - Loss: 0.3546\n",
            "Epoch 95/200 - Loss: 0.3544\n",
            "Epoch 96/200 - Loss: 0.3542\n",
            "Epoch 97/200 - Loss: 0.3541\n",
            "Epoch 98/200 - Loss: 0.3539\n",
            "Epoch 99/200 - Loss: 0.3537\n",
            "Epoch 100/200 - Loss: 0.3535\n",
            "Epoch 101/200 - Loss: 0.3533\n",
            "Epoch 102/200 - Loss: 0.3532\n",
            "Epoch 103/200 - Loss: 0.3530\n",
            "Epoch 104/200 - Loss: 0.3528\n",
            "Epoch 105/200 - Loss: 0.3526\n",
            "Epoch 106/200 - Loss: 0.3524\n",
            "Epoch 107/200 - Loss: 0.3523\n",
            "Epoch 108/200 - Loss: 0.3521\n",
            "Epoch 109/200 - Loss: 0.3519\n",
            "Epoch 110/200 - Loss: 0.3517\n",
            "Epoch 111/200 - Loss: 0.3516\n",
            "Epoch 112/200 - Loss: 0.3514\n",
            "Epoch 113/200 - Loss: 0.3512\n",
            "Epoch 114/200 - Loss: 0.3510\n",
            "Epoch 115/200 - Loss: 0.3509\n",
            "Epoch 116/200 - Loss: 0.3507\n",
            "Epoch 117/200 - Loss: 0.3505\n",
            "Epoch 118/200 - Loss: 0.3504\n",
            "Epoch 119/200 - Loss: 0.3502\n",
            "Epoch 120/200 - Loss: 0.3500\n",
            "Epoch 121/200 - Loss: 0.3498\n",
            "Epoch 122/200 - Loss: 0.3497\n",
            "Epoch 123/200 - Loss: 0.3495\n",
            "Epoch 124/200 - Loss: 0.3493\n",
            "Epoch 125/200 - Loss: 0.3492\n",
            "Epoch 126/200 - Loss: 0.3490\n",
            "Epoch 127/200 - Loss: 0.3488\n",
            "Epoch 128/200 - Loss: 0.3487\n",
            "Epoch 129/200 - Loss: 0.3485\n",
            "Epoch 130/200 - Loss: 0.3483\n",
            "Epoch 131/200 - Loss: 0.3482\n",
            "Epoch 132/200 - Loss: 0.3480\n",
            "Epoch 133/200 - Loss: 0.3478\n",
            "Epoch 134/200 - Loss: 0.3477\n",
            "Epoch 135/200 - Loss: 0.3475\n",
            "Epoch 136/200 - Loss: 0.3473\n",
            "Epoch 137/200 - Loss: 0.3472\n",
            "Epoch 138/200 - Loss: 0.3470\n",
            "Epoch 139/200 - Loss: 0.3468\n",
            "Epoch 140/200 - Loss: 0.3467\n",
            "Epoch 141/200 - Loss: 0.3465\n",
            "Epoch 142/200 - Loss: 0.3463\n",
            "Epoch 143/200 - Loss: 0.3462\n",
            "Epoch 144/200 - Loss: 0.3460\n",
            "Epoch 145/200 - Loss: 0.3459\n",
            "Epoch 146/200 - Loss: 0.3457\n",
            "Epoch 147/200 - Loss: 0.3455\n",
            "Epoch 148/200 - Loss: 0.3454\n",
            "Epoch 149/200 - Loss: 0.3452\n",
            "Epoch 150/200 - Loss: 0.3451\n",
            "Epoch 151/200 - Loss: 0.3449\n",
            "Epoch 152/200 - Loss: 0.3447\n",
            "Epoch 153/200 - Loss: 0.3446\n",
            "Epoch 154/200 - Loss: 0.3444\n",
            "Epoch 155/200 - Loss: 0.3443\n",
            "Epoch 156/200 - Loss: 0.3441\n",
            "Epoch 157/200 - Loss: 0.3439\n",
            "Epoch 158/200 - Loss: 0.3438\n",
            "Epoch 159/200 - Loss: 0.3436\n",
            "Epoch 160/200 - Loss: 0.3435\n",
            "Epoch 161/200 - Loss: 0.3433\n",
            "Epoch 162/200 - Loss: 0.3432\n",
            "Epoch 163/200 - Loss: 0.3430\n",
            "Epoch 164/200 - Loss: 0.3429\n",
            "Epoch 165/200 - Loss: 0.3427\n",
            "Epoch 166/200 - Loss: 0.3425\n",
            "Epoch 167/200 - Loss: 0.3424\n",
            "Epoch 168/200 - Loss: 0.3422\n",
            "Epoch 169/200 - Loss: 0.3421\n",
            "Epoch 170/200 - Loss: 0.3419\n",
            "Epoch 171/200 - Loss: 0.3418\n",
            "Epoch 172/200 - Loss: 0.3416\n",
            "Epoch 173/200 - Loss: 0.3415\n",
            "Epoch 174/200 - Loss: 0.3413\n",
            "Epoch 175/200 - Loss: 0.3412\n",
            "Epoch 176/200 - Loss: 0.3410\n",
            "Epoch 177/200 - Loss: 0.3409\n",
            "Epoch 178/200 - Loss: 0.3407\n",
            "Epoch 179/200 - Loss: 0.3406\n",
            "Epoch 180/200 - Loss: 0.3404\n",
            "Epoch 181/200 - Loss: 0.3403\n",
            "Epoch 182/200 - Loss: 0.3401\n",
            "Epoch 183/200 - Loss: 0.3400\n",
            "Epoch 184/200 - Loss: 0.3398\n",
            "Epoch 185/200 - Loss: 0.3397\n",
            "Epoch 186/200 - Loss: 0.3395\n",
            "Epoch 187/200 - Loss: 0.3394\n",
            "Epoch 188/200 - Loss: 0.3392\n",
            "Epoch 189/200 - Loss: 0.3391\n",
            "Epoch 190/200 - Loss: 0.3389\n",
            "Epoch 191/200 - Loss: 0.3388\n",
            "Epoch 192/200 - Loss: 0.3386\n",
            "Epoch 193/200 - Loss: 0.3385\n",
            "Epoch 194/200 - Loss: 0.3383\n",
            "Epoch 195/200 - Loss: 0.3382\n",
            "Epoch 196/200 - Loss: 0.3381\n",
            "Epoch 197/200 - Loss: 0.3379\n",
            "Epoch 198/200 - Loss: 0.3378\n",
            "Epoch 199/200 - Loss: 0.3376\n",
            "Epoch 200/200 - Loss: 0.3375\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 200\n",
        "lr = 0.03\n",
        "\n",
        "train(X_train_f, one_hot, n_epochs, lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIFP9cq_Z2WO"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xalU7I_3YOui"
      },
      "outputs": [],
      "source": [
        "def predict(X_test):\n",
        "    output, hidden = forward(X_test)\n",
        "    predictions = np.argmax(output, axis=0)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvxfFvftaMu3",
        "outputId": "54965c32-1e4b-4785-b03e-3691c59fbe48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set: 91.29%\n"
          ]
        }
      ],
      "source": [
        "predictions = predict(X_test_f)\n",
        "\n",
        "correct_predictions = np.sum(predictions == y_test)\n",
        "total_samples = len(y_test)\n",
        "accuracy = correct_predictions / total_samples\n",
        "\n",
        "print(f\"Accuracy on test set: {accuracy:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTwgw1Mv2sFc",
        "outputId": "eca73193-388e-4645-a6ae-341e435bb343"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 955,    0,    2,    2,    0,    6,   11,    1,    3,    0],\n",
              "       [   0, 1107,    1,    5,    0,    0,    4,    2,   16,    0],\n",
              "       [  10,    3,  901,   20,   19,    1,   14,   19,   38,    7],\n",
              "       [   4,    0,   21,  910,    2,   26,    2,   13,   22,   10],\n",
              "       [   1,    4,    4,    2,  911,    1,   10,    2,    9,   38],\n",
              "       [  12,    3,    4,   38,    8,  770,   17,    7,   27,    6],\n",
              "       [  14,    3,    4,    2,   15,   19,  897,    1,    3,    0],\n",
              "       [   4,   16,   25,    4,   13,    0,    0,  936,    5,   25],\n",
              "       [   9,    7,    9,   27,   10,   21,   12,   13,  850,   16],\n",
              "       [  10,    5,    3,   13,   45,   14,    1,   19,    7,  892]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "conf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWQ79q5TEFOV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXVSaNypCoWmP6NXI0an7U",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}